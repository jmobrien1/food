name: Debug Server

on:
  workflow_dispatch:

jobs:
  debug:
    runs-on: self-hosted
    defaults:
      run:
        working-directory: /home/jmobrien1/food

    steps:
      - name: GPU info
        run: |
          echo "=== nvidia-smi ==="
          nvidia-smi 2>/dev/null || echo "nvidia-smi not found"
          echo ""
          echo "=== lspci GPU ==="
          lspci 2>/dev/null | grep -i -E "vga|3d|nvidia" || echo "no GPU in lspci"
          echo ""
          echo "=== CUDA version ==="
          nvcc --version 2>/dev/null || echo "nvcc not found"

      - name: Ollama GPU usage
        run: |
          echo "=== Ollama ps (loaded models) ==="
          curl -s http://localhost:11434/api/ps | python3 -m json.tool 2>/dev/null || echo "No models loaded"
          echo ""
          echo "=== Ollama logs (last 20) ==="
          journalctl -u ollama --no-pager -n 20 2>/dev/null || sudo journalctl -u ollama --no-pager -n 20 2>/dev/null || echo "No ollama journal logs"

      - name: Ollama install info
        run: |
          echo "=== Ollama version ==="
          ollama --version 2>/dev/null || echo "ollama CLI not found"
          echo ""
          echo "=== Ollama binary ==="
          which ollama 2>/dev/null || echo "not in PATH"
          echo ""
          echo "=== Ollama service ==="
          systemctl status ollama 2>/dev/null | head -15 || echo "No systemd service"

      - name: Quick generation test with timing
        run: |
          echo "=== Timed Ollama generation ==="
          time curl -s --max-time 120 http://localhost:11434/api/generate \
            -d '{"model":"qwen2.5","prompt":"Say hello in one word","stream":false}' \
            | python3 -c "import sys,json; r=json.load(sys.stdin); print('Response:', r.get('response','NONE')); print('Eval count:', r.get('eval_count','?')); print('Eval duration (ms):', round(r.get('eval_duration',0)/1e6, 1)); print('Prompt eval (ms):', round(r.get('prompt_eval_duration',0)/1e6, 1))" \
            || echo "Generation failed"
